{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "\"\"\"Imports das funções de chamadas das tabelas\"\"\"\n",
    "from etl_pyspark import reader_csv,executa_consulta_tabela1,reader_table,executa_consulta_tabela2\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Java\\jdk-19\"\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\databases_Etl\\venv\\Lib\\site-packages\\pyspark\"\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\databases_Etl\\venv\\Lib\\site-packages\\pyspark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__=='__main__':\n",
    "    conf = SparkConf()\n",
    "    conf.set(\"spark.master\",\"local[*]\")\n",
    "    conf.set(\"spark.executor.memory\", \"4g\")\n",
    "    conf.set(\"spark.driver.memory\", \"4g\")\n",
    "    conf.set(\"spark.sql.adaptive.enabled\",\"true\")\n",
    "    conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\",\"true\")\n",
    "    conf.set(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "    conf.set(\"spark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled\",\"true\")\n",
    "    conf.set(\"spark.sql.adaptive.skewJoin.enabled\",\"true\")\n",
    "    conf.set(\"spark.sql.statistics.size.autoUpdate.enabled\",\"true\")\n",
    "    conf.set(\"spark.sql.inMemoryColumnarStorage.compressed\",\"true\")\n",
    "    conf.set(\"hive.exec.dynamic.partition\", \"true\")\n",
    "    conf.set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "    conf.set(\"spark.sql.ansi.enabled\",\"true\")\n",
    "    conf.set('spark.driver.extraClassPath', r\"C:\\leads\\venv\\Lib\\site-packages\\pyspark\\mssql-jdbc-9.4.0.jre11.jar\")\n",
    "    conf.set('spark.executor.extraClassPath', r\"C:\\leads\\venv\\Lib\\site-packages\\pyspark\\mssql-jdbc-9.4.0.jre11.jar\")\n",
    "    spark = SparkSession.builder\\\n",
    "            .config(conf=conf)\\\n",
    "            .config(\"spark.sql.warehouse.dir\", warehouse_location)\\\n",
    "            .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "            .enableHiveSupport() \\\n",
    "            .getOrCreate()\n",
    "    \n",
    "    reader_csv(spark)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
