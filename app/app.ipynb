{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = r\"/usr/lib/jvm/java-1.11.0-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = r\"\\\\wsl.localhost\\Ubuntu-18.04\\home\\guilherme\\als_recomendacao_spark_app\\spark\"\n",
    "os.environ[\"LIVY_CONF_DIR\"] = r\"\\\\wsl.localhost\\Ubuntu-18.04\\home\\guilherme\\als_recomendacao_spark_app\\livy_server\\apache-livy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] O sistema nÃ£o pode encontrar o arquivo especificado",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m\n\u001b[0;32m     16\u001b[0m conf\u001b[39m.\u001b[39mset(\u001b[39m'\u001b[39m\u001b[39mspark.driver.extraClassPath\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mleads\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mvenv\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mLib\u001b[39m\u001b[39m\\\u001b[39m\u001b[39msite-packages\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mpyspark\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mmssql-jdbc-9.4.0.jre11.jar\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m conf\u001b[39m.\u001b[39mset(\u001b[39m'\u001b[39m\u001b[39mspark.executor.extraClassPath\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mleads\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mvenv\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mLib\u001b[39m\u001b[39m\\\u001b[39m\u001b[39msite-packages\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mpyspark\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mmssql-jdbc-9.4.0.jre11.jar\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\\\n\u001b[0;32m     19\u001b[0m         \u001b[39m.\u001b[39;49mconfig(conf\u001b[39m=\u001b[39;49mconf)\\\n\u001b[0;32m     20\u001b[0m         \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.sql.warehouse.dir\u001b[39;49m\u001b[39m\"\u001b[39;49m, warehouse_location)\\\n\u001b[0;32m     21\u001b[0m         \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.sql.catalogImplementation\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mhive\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m     22\u001b[0m         \u001b[39m.\u001b[39;49menableHiveSupport() \\\n\u001b[1;32m---> 23\u001b[0m         \u001b[39m.\u001b[39;49mgetOrCreate()\n\u001b[0;32m     25\u001b[0m reader_csv(spark)\n",
      "File \u001b[1;32mc:\\Users\\Guilh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:477\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    475\u001b[0m     sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[0;32m    476\u001b[0m \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[0;32m    478\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Users\\Guilh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:512\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    511\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 512\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[0;32m    513\u001b[0m     \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    514\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\Guilh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:198\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    194\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m     )\n\u001b[1;32m--> 198\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[0;32m    201\u001b[0m         master,\n\u001b[0;32m    202\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    213\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Guilh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:432\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    431\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[1;32m--> 432\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    433\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[0;32m    435\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Users\\Guilh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\java_gateway.py:99\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m     proc \u001b[39m=\u001b[39m Popen(command, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpopen_kwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m     \u001b[39m# preexec_fn not supported on Windows\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m     proc \u001b[39m=\u001b[39m Popen(command, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpopen_kwargs)\n\u001b[0;32m    101\u001b[0m \u001b[39m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m proc\u001b[39m.\u001b[39mpoll() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n",
      "File \u001b[1;32mc:\\Users\\Guilh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:1024\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1020\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_mode:\n\u001b[0;32m   1021\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[0;32m   1022\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[1;32m-> 1024\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0;32m   1025\u001b[0m                         pass_fds, cwd, env,\n\u001b[0;32m   1026\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[0;32m   1027\u001b[0m                         p2cread, p2cwrite,\n\u001b[0;32m   1028\u001b[0m                         c2pread, c2pwrite,\n\u001b[0;32m   1029\u001b[0m                         errread, errwrite,\n\u001b[0;32m   1030\u001b[0m                         restore_signals,\n\u001b[0;32m   1031\u001b[0m                         gid, gids, uid, umask,\n\u001b[0;32m   1032\u001b[0m                         start_new_session, process_group)\n\u001b[0;32m   1033\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m   1034\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdin, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mc:\\Users\\Guilh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:1493\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1491\u001b[0m \u001b[39m# Start the process\u001b[39;00m\n\u001b[0;32m   1492\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1493\u001b[0m     hp, ht, pid, tid \u001b[39m=\u001b[39m _winapi\u001b[39m.\u001b[39;49mCreateProcess(executable, args,\n\u001b[0;32m   1494\u001b[0m                              \u001b[39m# no special security\u001b[39;49;00m\n\u001b[0;32m   1495\u001b[0m                              \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   1496\u001b[0m                              \u001b[39mint\u001b[39;49m(\u001b[39mnot\u001b[39;49;00m close_fds),\n\u001b[0;32m   1497\u001b[0m                              creationflags,\n\u001b[0;32m   1498\u001b[0m                              env,\n\u001b[0;32m   1499\u001b[0m                              cwd,\n\u001b[0;32m   1500\u001b[0m                              startupinfo)\n\u001b[0;32m   1501\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1502\u001b[0m     \u001b[39m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m     \u001b[39m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1506\u001b[0m     \u001b[39m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1507\u001b[0m     \u001b[39m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1508\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1509\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1510\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] O sistema nÃ£o pode encontrar o arquivo especificado"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__=='__main__':\n",
    "    conf = SparkConf()\n",
    "    conf.set(\"spark.master\",\"local[*]\")\n",
    "    conf.set(\"spark.executor.memory\", \"4g\")\n",
    "    conf.set(\"spark.driver.memory\", \"4g\")\n",
    "    conf.set(\"spark.sql.adaptive.enabled\",\"true\")\n",
    "    conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\",\"true\")\n",
    "    conf.set(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "    conf.set(\"spark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled\",\"true\")\n",
    "    conf.set(\"spark.sql.adaptive.skewJoin.enabled\",\"true\")\n",
    "    conf.set(\"spark.sql.statistics.size.autoUpdate.enabled\",\"true\")\n",
    "    conf.set(\"spark.sql.inMemoryColumnarStorage.compressed\",\"true\")\n",
    "    conf.set(\"hive.exec.dynamic.partition\", \"true\")\n",
    "    conf.set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "    conf.set(\"spark.sql.ansi.enabled\",\"true\")\n",
    "    conf.set('spark.driver.extraClassPath', r\"C:\\leads\\venv\\Lib\\site-packages\\pyspark\\mssql-jdbc-9.4.0.jre11.jar\")\n",
    "    conf.set('spark.executor.extraClassPath', r\"C:\\leads\\venv\\Lib\\site-packages\\pyspark\\mssql-jdbc-9.4.0.jre11.jar\")\n",
    "    spark = SparkSession.builder\\\n",
    "            .config(conf=conf)\\\n",
    "            .config(\"spark.sql.warehouse.dir\", warehouse_location)\\\n",
    "            .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "            .enableHiveSupport() \\\n",
    "            .getOrCreate()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
