{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import SparkSession ,Row\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import abspath\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import os\n",
    "import findspark\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#databricks tutorialBase\n",
    "#https://docs.databricks.com/external-data/sql-server.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = r\"/usr/lib/jvm/java-1.11.0-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = r\"/home/guilherme/als_recomendacao_spark_app/spark/spark\"\n",
    "os.environ[\"LIVY_CONF_DIR\"] = r\"/home/guilherme/als_recomendacao_spark_app/livy_server\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warehouse_location = abspath('spark-warehouse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/10 00:55:41 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.master\",\"local[*]\")\n",
    "conf.set(\"spark.executor.memory\", \"4g\")\n",
    "conf.set(\"spark.driver.memory\", \"4g\")\n",
    "conf.set(\"spark.sql.adaptive.enabled\",\"true\")\n",
    "conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\",\"true\")\n",
    "conf.set(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "conf.set(\"spark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled\",\"true\")\n",
    "conf.set(\"spark.sql.adaptive.skewJoin.enabled\",\"true\")\n",
    "conf.set(\"spark.sql.statistics.size.autoUpdate.enabled\",\"true\")\n",
    "conf.set(\"spark.sql.inMemoryColumnarStorage.compressed\",\"true\")\n",
    "conf.set(\"hive.exec.dynamic.partition\", \"true\")\n",
    "conf.set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "conf.set(\"spark.sql.ansi.enabled\",\"true\")\n",
    "conf.set('spark.driver.extraClassPath', r\"/home/guilherme/als_recomendacao_spark_app/sqlserverjars/mssql-jdbc-12.2.0.jre11.jar\")\n",
    "conf.set('spark.executor.extraClassPath', r\"/home/guilherme/als_recomendacao_spark_app/sqlserverjars/mssql-jdbc-12.2.0.jre11.jar\")\n",
    "spark = SparkSession.builder\\\n",
    "        .config(conf=conf)\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse_location)\\\n",
    "        .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "\n",
    "database_host = \"BIGDATA\"\n",
    "database_port = \"1433\" # update if you use a non-default port\n",
    "database_name = \"BIGDATA\"\n",
    "table = \"comercial.ranking_atendimento_tim\"\n",
    "user = \"sa\"\n",
    "password = \"123\"\n",
    "\n",
    "url = f\"jdbc:sqlserver://{database_host}:{database_port};database={database_name}\"\n",
    "\n",
    "remote_table = (spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"driver\", driver)\n",
    "  .option(\"url\", url)\n",
    "  .option(\"dbtable\", table)\n",
    "  .option(\"user\", user)\n",
    "  .option(\"password\", password)\n",
    "  .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
